{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bee50dd-8b3c-4143-959d-541bf1a11766",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 00:18:49.010042: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import logging, sys\n",
    "logging.disable(sys.maxsize)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from types import SimpleNamespace\n",
    "from extraction import *\n",
    "import scipy.sparse as sp\n",
    "from aind_ophys_utils.summary_images import pnr_image\n",
    "from multiprocessing.pool import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0601085-36ba-497f-86cd-84911313e468",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = SimpleNamespace(\n",
    "        input_dir=\"../data/\",\n",
    "        output_dir=\"../results/\",\n",
    "        tmp_dir=\"/scratch\",\n",
    "        diameter=0,\n",
    "        anatomical_only=2,\n",
    "        denoise=False,\n",
    "        cellprob_threshold=0.0,\n",
    "        flow_threshold=1.5,\n",
    "        spatial_hp_cp=0,\n",
    "        pretrained_model=\"cyto\",\n",
    "        neuropil=\"CNMF\",\n",
    "        rf=100, stride=20, K=20, merge_thr=.6, ssub=2, tsub=2, normalize_init=True, init='greedy_roi',\n",
    "        contour_video=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e1e320d-fb1b-48db-84ba-ed210ca36494",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    output_dir = Path(args.output_dir).resolve()\n",
    "    input_dir = Path(args.input_dir).resolve()\n",
    "    tmp_dir = Path(args.tmp_dir).resolve()\n",
    "    session, data_description, subject = get_metdata(input_dir)\n",
    "    subject_id = subject.get(\"subject_id\", \"\")\n",
    "    name = data_description.get(\"name\", \"\")\n",
    "    setup_logging(\"aind-ophys-extraction-suite2p\", mouse_id=subject_id, session_name=name)\n",
    "    if next(input_dir.rglob(\"*decrosstalk.h5\"), \"\"):\n",
    "        input_fn = next(input_dir.rglob(\"*decrosstalk.h5\"))\n",
    "    else:\n",
    "        input_fn = next(input_dir.rglob(\"*registered.h5\"))\n",
    "    parent_directory = input_fn.parent\n",
    "    if session is not None and \"Bergamo\" in session[\"rig_id\"]:\n",
    "        motion_corrected_fn = bergamo_segmentation(input_fn, session, temp_dir=tmp_dir)\n",
    "    else:\n",
    "        motion_corrected_fn = input_fn\n",
    "    if not data_description or \"multiplane\" in data_description.get(\"name\", \"\"):\n",
    "        unique_id = motion_corrected_fn.parent.parent.name\n",
    "    else:\n",
    "        unique_id = \"_\".join(str(data_description[\"name\"]).split(\"_\")[-3:])\n",
    "\n",
    "    frame_rate = get_frame_rate(session)\n",
    "\n",
    "    output_dir = make_output_directory(output_dir, unique_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "807e11e5-32f9-4dbf-b620-a74f4814025d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "big_fn = \"/data/single-plane-ophys-harvard_2024-10-21_10_00_31_processed_2024-12-11_05-52-00/motion_correction/pophys_registered.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5104331d-6059-434b-b19a-cbe1c781e6a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 114 ms, sys: 1.05 s, total: 1.16 s\n",
      "Wall time: 39.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "os.environ['CAIMAN_TEMP'] = args.tmp_dir\n",
    "# create mmap file\n",
    "with h5py.File(big_fn) as fin:\n",
    "    data = fin[\"data\"]\n",
    "    if data.nbytes < 1e9:\n",
    "        fname_new = caiman.save_memmap([big_fn], var_name_hdf5='data',\n",
    "                                       order='C', base_name=unique_id)\n",
    "    else:\n",
    "        chunksize = 50\n",
    "        chunkfiles = [args.tmp_dir + f\"/chunk{k}.h5\" for k in range(0, data.shape[0], chunksize)]\n",
    "        def write_chunk(k):\n",
    "            with h5py.File(args.tmp_dir + f\"/chunk{k}.h5\", \"w\") as f:\n",
    "                f.create_dataset('data', data=data[k:k+chunksize])\n",
    "        with Pool() as pool:  # restricting to CO_CPUs does not speed things up\n",
    "            pool.map(write_chunk, range(0, data.shape[0],chunksize))\n",
    "            fname_new = caiman.save_memmap(chunkfiles, var_name_hdf5='data',\n",
    "                                       order='C', dview=pool, base_name=unique_id, n_chunks=16)\n",
    "            pool.map(os.remove, chunkfiles)  # remove temporary files\n",
    "        # # remove temporary files\n",
    "        # for cf in chunkfiles:\n",
    "        #     os.remove(cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "028aa1da-1440-4d9d-855e-74aca565a8b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.9 ms, sys: 916 ms, total: 944 ms\n",
      "Wall time: 44.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "os.environ['CAIMAN_TEMP'] = args.tmp_dir\n",
    "# create mmap file\n",
    "with h5py.File(big_fn) as fin:\n",
    "    data = fin[\"data\"]\n",
    "    if data.nbytes < 1e9:\n",
    "        fname_new = caiman.save_memmap([big_fn], var_name_hdf5='data',\n",
    "                                       order='C', base_name=unique_id)\n",
    "    else:\n",
    "        chunksize = 50\n",
    "        chunkfiles = [args.tmp_dir + f\"/chunk{k}.h5\" for k in range(0, data.shape[0], chunksize)]\n",
    "        def write_chunk(k):\n",
    "            with h5py.File(args.tmp_dir + f\"/chunk{k}.h5\", \"w\") as f:\n",
    "                f.create_dataset('data', data=data[k:k+chunksize])\n",
    "        with Pool(4) as pool:  # restricting to CO_CPUs does not speed things up\n",
    "            pool.map(write_chunk, range(0, data.shape[0],chunksize))\n",
    "            chunkfiles = [args.tmp_dir + f\"/chunk{k}.h5\" for k in range(0, data.shape[0],chunksize)]\n",
    "            fname_new = caiman.save_memmap(chunkfiles, var_name_hdf5='data',\n",
    "                                       order='C', dview=pool, base_name=unique_id, n_chunks=16)\n",
    "            pool.map(os.remove, chunkfiles)  # remove temporary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d301bccd-b670-4ff2-a4c8-0796cec758e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
